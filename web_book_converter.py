import gradio as gr
import os
import sys
import torch
import torchaudio
import time
import subprocess
from datetime import datetime

# è®¾ç½®ç¯å¢ƒè·¯å¾„
current_dir = os.getcwd()
# å›é€€åˆ°æ ‡å‡†çš„ CosyVoice ç›®å½•
cosyvoice_root = os.path.join(current_dir, 'CosyVoice')
sys.path.append(cosyvoice_root)
sys.path.append(os.path.join(cosyvoice_root, 'third_party', 'Matcha-TTS'))
# å¦‚æœæœ‰ ffmpegï¼Œæ·»åŠ è·¯å¾„ (å‡è®¾åœ¨ ffmpeg/bin ä¸‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¿½ç•¥)
ffmpeg_path = os.path.join(cosyvoice_root, 'ffmpeg', 'bin')
if os.path.exists(ffmpeg_path):
    os.environ["PATH"] = ffmpeg_path + os.pathsep + os.environ["PATH"]

# ç¦ç”¨ DeepSpeed æ£€æŸ¥
os.environ["DS_SKIP_CUDA_CHECK"] = "1"
os.environ["DS_BUILD_OPS"] = "0"

# å…¨å±€æ¨¡å‹å˜é‡
cosyvoice_model = None

# èµ„æºç›®å½•
ASSETS_DIR = os.path.join(current_dir, 'assets')
if not os.path.exists(ASSETS_DIR):
    os.makedirs(ASSETS_DIR)

def get_reference_audio_list():
    """æ‰«æ assets ç›®å½•ä¸‹çš„éŸ³é¢‘æ–‡ä»¶"""
    files = []
    valid_exts = ['.wav', '.mp3', '.flac']
    if os.path.exists(ASSETS_DIR):
        for f in os.listdir(ASSETS_DIR):
            if any(f.lower().endswith(ext) for ext in valid_exts):
                files.append(f)
    return sorted(files)

def get_prompt_text_for_audio(audio_filename):
    """å°è¯•è¯»å–åŒå txt æ–‡ä»¶çš„å†…å®¹"""
    if not audio_filename:
        return ""
    
    base_name = os.path.splitext(audio_filename)[0]
    txt_path = os.path.join(ASSETS_DIR, base_name + '.txt')
    
    if os.path.exists(txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except:
            return ""
    return ""

def load_model():
    global cosyvoice_model
    if cosyvoice_model is None:
        try:
            # å°è¯•é€‚é…ä¸åŒçš„ä»£ç ç‰ˆæœ¬
            try:
                from cosyvoice.cli.cosyvoice import CosyVoice2 as CosyVoiceCls
                print("Detected CosyVoice2 class.")
            except ImportError:
                from cosyvoice.cli.cosyvoice import CosyVoice as CosyVoiceCls
                print("Detected CosyVoice class.")

            # æŒ‡å‘åˆšä¸‹è½½çš„ CosyVoice2-0.5B
            model_dir = os.path.join(cosyvoice_root, 'pretrained_models', 'CosyVoice2-0.5B')
            print(f"Loading model from {model_dir}...")
            
            try:
                # å°è¯•åŠ è½½
                cosyvoice_model = CosyVoiceCls(model_dir, load_jit=False, load_trt=False, load_vllm=False, fp16=True)
                return "Model loaded successfully (FP16)."
            except Exception as e:
                print(f"FP16 load failed: {e}, trying FP32...")
                cosyvoice_model = CosyVoiceCls(model_dir, load_jit=False, load_trt=False, load_vllm=False, fp16=False)
                return "Model loaded successfully (FP32)."
        except Exception as e:
            return f"Error loading model: {str(e)}"
    return "Model already loaded."

def convert_book(text_files, ref_audio_name, prompt_text, progress=gr.Progress(track_tqdm=True)):
    global cosyvoice_model
    
    # 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    if cosyvoice_model is None:
        progress(0, desc="Loading model...")
        yield "Loading model...", None
        msg = load_model()
        yield msg, None
        if cosyvoice_model is None:
            return

    # 2. éªŒè¯è¾“å…¥
    progress(0.1, desc="Validating inputs...")
    yield "Validating inputs...", None
    if not text_files:
        yield "Error: Please upload at least one text file.", None
        return
    
    if not ref_audio_name:
        yield "Error: Please select a reference audio.", None
        return

    ref_audio_path = os.path.join(ASSETS_DIR, ref_audio_name)
    if not os.path.exists(ref_audio_path):
        yield f"Error: Audio file not found: {ref_audio_path}", None
        return

    # ç¡®ä¿ text_files æ˜¯åˆ—è¡¨
    if not isinstance(text_files, list):
        text_files = [text_files]

    total_files = len(text_files)
    all_generated_files = []
    last_mp4_path = None

    try:
        for file_idx, text_file in enumerate(text_files):
            file_name = os.path.basename(text_file.name)
            progress((file_idx) / total_files, desc=f"Processing file {file_idx + 1}/{total_files}: {file_name}")
            yield f"Processing file {file_idx + 1}/{total_files}: {file_name}...", None

            # è¯»å–æ–‡æœ¬
            with open(text_file.name, 'r', encoding='utf-8') as f:
                full_text = f.read().strip()
            
            yield f"File {file_idx + 1}/{total_files}: Text loaded ({len(full_text)} chars). Inferencing...", None
            
            from cosyvoice.utils.file_utils import load_wav
            prompt_speech_16k = load_wav(ref_audio_path, 16000)

            all_audio = []
            start_time = time.time()
            
            # 3. å¼€å§‹æ¨ç†
            chunk_count = 0
            # ç²—ç•¥ä¼°ç®—æ€» chunks æ•°ï¼šå‡è®¾æ¯ 10 ä¸ªå­—ç¬¦ç”Ÿæˆä¸€ä¸ª chunkï¼ˆæ ¹æ®ç»éªŒå€¼è°ƒæ•´ï¼‰
            estimated_chunks = max(1, len(full_text) // 10)
            
            for i, output in enumerate(cosyvoice_model.inference_zero_shot(full_text, prompt_text, prompt_speech_16k, stream=False)):
                chunk_count += 1
                duration = output['tts_speech'].shape[1] / 24000
                msg = f"File {file_idx + 1}/{total_files}: Generated chunk {chunk_count} ({duration:.2f}s)..."
                yield msg, None
                
                # è®¡ç®—å½“å‰æ–‡ä»¶å†…çš„è¿›åº¦ (0.0 - 0.9)ï¼Œé¢„ç•™ 0.1 ç»™è§†é¢‘è½¬æ¢
                file_progress = min(0.9, chunk_count / estimated_chunks)
                global_progress = (file_idx + file_progress) / total_files
                progress(global_progress, desc=msg)
                
                all_audio.append(output['tts_speech'])

            if not all_audio:
                yield f"File {file_idx + 1}/{total_files}: Error: No audio generated for {file_name}", None
                continue

            # 4. å¤„ç†ç»“æœï¼ˆæŒ‰æœ€å¤§æ—¶é•¿æ‹†åˆ†ï¼‰
            MAX_DURATION_SEC = 45 * 60  # 45 minutes
            
            full_audio_tensor = torch.cat(all_audio, dim=1)
            total_samples = full_audio_tensor.shape[1]
            sample_rate = cosyvoice_model.sample_rate
            max_samples = MAX_DURATION_SEC * sample_rate
            
            num_parts = (total_samples + max_samples - 1) // max_samples
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºè¾“å‡ºå‰ç¼€
            base_filename = os.path.splitext(file_name)[0]
            
            for i in range(num_parts):
                progress((file_idx + (i / num_parts)) / total_files, desc=f"Converting part {i+1}/{num_parts} to video...")
                
                start = i * max_samples
                end = min((i + 1) * max_samples, total_samples)
                part_tensor = full_audio_tensor[:, start:end]
                
                part_suffix = f"_part{i+1}" if num_parts > 1 else ""
                output_base = f"{base_filename}_{timestamp}{part_suffix}"
                
                temp_wav = os.path.join(current_dir, f"temp_{output_base}.wav")
                output_mp4 = f"{output_base}.mp4"
                mp4_path = os.path.join(current_dir, output_mp4)
                
                torchaudio.save(temp_wav, part_tensor, sample_rate)
                
                # 5. ç”Ÿæˆè§†é¢‘ (FFmpeg)
                yield f"File {file_idx + 1}/{total_files}: Converting part {i+1}/{num_parts} to video...", None
                
                cmd = [
                    "ffmpeg", "-y",
                    "-f", "lavfi", "-i", "color=c=black:s=320x240:r=1",
                    "-i", temp_wav,
                    "-c:v", "libx264", "-tune", "stillimage", "-pix_fmt", "yuv420p", "-crf", "40", "-preset", "veryfast",
                    "-c:a", "aac", "-b:a", "64k",
                    "-shortest",
                    mp4_path
                ]
                
                process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                if os.path.exists(temp_wav):
                    os.remove(temp_wav)

                if process.returncode != 0:
                     print(f"FFmpeg error part {i+1}: {process.stderr.decode()}")
                     yield f"File {file_idx + 1}/{total_files}: Video generation failed for part {i+1}.", None
                else:
                    all_generated_files.append(output_mp4)
            
            file_time = time.time() - start_time
            yield f"File {file_idx + 1}/{total_files}: Done ({file_time:.2f}s)", all_generated_files

        progress(1.0, desc="All done!")
        msg = f"All done! Generated {len(all_generated_files)} file(s):\n" + "\n".join(all_generated_files)
        yield msg, all_generated_files

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield f"Error: {str(e)}", None

def refresh_audio_list():
    return gr.Dropdown(choices=get_reference_audio_list())

# Gradio ç•Œé¢æ„å»º
with gr.Blocks(title="CosyVoice Book Converter") as demo:
    gr.Markdown("# ğŸ“š CosyVoice æœ‰å£°ä¹¦è½¬æ¢å™¨")
    gr.Markdown("ä¸Šä¼  txt æ–‡æœ¬ï¼Œé€‰æ‹©é¢„è®¾çš„å‚è€ƒéŸ³é¢‘ï¼Œä¸€é”®ç”Ÿæˆæœ‰å£°ä¹¦è§†é¢‘ã€‚")
    
    with gr.Row():
        with gr.Column():
            text_input = gr.File(label="ä¸Šä¼ ä¹¦ç± (.txt)", file_types=[".txt"], file_count="multiple")
            
            with gr.Group():
                gr.Markdown("### å‚è€ƒéŸ³é¢‘è®¾ç½®")
                with gr.Row():
                    ref_audio_dropdown = gr.Dropdown(
                        label="é€‰æ‹©å‚è€ƒéŸ³é¢‘ (æ¥è‡ª assets æ–‡ä»¶å¤¹)", 
                        choices=get_reference_audio_list(),
                        value=get_reference_audio_list()[0] if get_reference_audio_list() else None,
                        interactive=True
                    )
                    refresh_btn = gr.Button("ğŸ”„", size="sm", scale=0)
                
                prompt_text_input = gr.Textbox(
                    label="å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ (Prompt Text)", 
                    lines=2,
                    placeholder="é€‰æ‹©éŸ³é¢‘åè‡ªåŠ¨å¡«å……..."
                )
            
            with gr.Row():
                convert_btn = gr.Button("å¼€å§‹è½¬æ¢", variant="primary")
                stop_btn = gr.Button("åœæ­¢è½¬æ¢", variant="stop")
        
        with gr.Column():
            log_output = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=10, interactive=False)
            # video_output removed
            files_output = gr.File(label="æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶ä¸‹è½½", file_count="multiple", interactive=False)

    # äº‹ä»¶ç»‘å®š
    refresh_btn.click(fn=refresh_audio_list, inputs=[], outputs=ref_audio_dropdown)
    
    # é€‰æ‹©éŸ³é¢‘æ—¶è‡ªåŠ¨æ›´æ–° prompt text
    ref_audio_dropdown.change(
        fn=get_prompt_text_for_audio,
        inputs=[ref_audio_dropdown],
        outputs=[prompt_text_input]
    )

    submit_event = convert_btn.click(
        fn=convert_book,
        inputs=[text_input, ref_audio_dropdown, prompt_text_input],
        outputs=[log_output, files_output]
    )
    
    stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[submit_event])
    
    # åˆå§‹åŒ–æ—¶å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªéŸ³é¢‘çš„æ–‡æœ¬
    demo.load(
        fn=get_prompt_text_for_audio,
        inputs=[ref_audio_dropdown], 
        outputs=[prompt_text_input]
    )

if __name__ == "__main__":
    print("Starting Web UI...")
    # ä½¿ç”¨ 0.0.0.0 è®©æœåŠ¡åœ¨æ‰€æœ‰ç½‘ç»œæ¥å£ä¸Šç›‘å¬
    import gradio
    # å°è¯•ç¦ç”¨ localhost æ£€æŸ¥
    gradio.strings.en["SHARE_LINK_MESSAGE"] = ""
    try:
        demo.queue().launch(
            server_name="0.0.0.0", 
            server_port=7860, 
            show_error=True,
            quiet=False,
            _frontend=False  # ç¦ç”¨å‰ç«¯æ£€æŸ¥
        )
    except ValueError as e:
        if "shareable link" in str(e):
            print("Fallback: Using share=True due to network restrictions")
            demo.queue().launch(server_name="0.0.0.0", server_port=7860, show_error=True, share=True)
