import gradio as gr
import os
import sys
import torch
import torchaudio
import time
import subprocess
import threading
import json
from datetime import datetime

# è®¾ç½®ç¯å¢ƒè·¯å¾„
current_dir = os.getcwd()
# å›é€€åˆ°æ ‡å‡†çš„ CosyVoice ç›®å½•
cosyvoice_root = os.path.join(current_dir, 'CosyVoice')
sys.path.append(cosyvoice_root)
sys.path.append(os.path.join(cosyvoice_root, 'third_party', 'Matcha-TTS'))
# å¦‚æœæœ‰ ffmpegï¼Œæ·»åŠ è·¯å¾„ (å‡è®¾åœ¨ ffmpeg/bin ä¸‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¿½ç•¥)
ffmpeg_path = os.path.join(cosyvoice_root, 'ffmpeg', 'bin')
if os.path.exists(ffmpeg_path):
    os.environ["PATH"] = ffmpeg_path + os.pathsep + os.environ["PATH"]

# ç¦ç”¨ DeepSpeed æ£€æŸ¥
os.environ["DS_SKIP_CUDA_CHECK"] = "1"
os.environ["DS_BUILD_OPS"] = "0"

# å…¨å±€æ¨¡å‹å˜é‡
cosyvoice_model = None

# åœæ­¢æ ‡å¿—
stop_flag = threading.Event()
current_inference_thread = None
# åå°ä»»åŠ¡çº¿ç¨‹
background_task_thread = None
# ä»»åŠ¡ç®¡ç†é”ï¼šé˜²æ­¢å¹¶å‘è¯·æ±‚å¯¼è‡´ background_task_thread è¢«è¦†ç›–
background_task_lock = threading.Lock()

# èµ„æºç›®å½•
ASSETS_DIR = os.path.join(current_dir, 'assets')
if not os.path.exists(ASSETS_DIR):
    os.makedirs(ASSETS_DIR)

# è¾“å‡ºç›®å½•ï¼ˆç”Ÿæˆçš„æ–‡ä»¶ä¿å­˜åˆ°è¿™é‡Œï¼‰
OUTPUT_DIR = os.path.join(current_dir, 'output')
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ä»»åŠ¡çŠ¶æ€æ–‡ä»¶
TASK_STATE_FILE = os.path.join(current_dir, 'task_state.json')
# æ–‡ä»¶I/Oé”ï¼Œé˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´æ–‡ä»¶æŸå
task_state_lock = threading.Lock()

def get_reference_audio_list():
    """æ‰«æ assets ç›®å½•ä¸‹çš„éŸ³é¢‘æ–‡ä»¶"""
    files = []
    valid_exts = ['.wav', '.mp3', '.flac']
    if os.path.exists(ASSETS_DIR):
        for f in os.listdir(ASSETS_DIR):
            if any(f.lower().endswith(ext) for ext in valid_exts):
                files.append(f)
    return sorted(files)

def get_prompt_text_for_audio(audio_filename):
    """å°è¯•è¯»å–åŒå txt æ–‡ä»¶çš„å†…å®¹"""
    if not audio_filename:
        return ""
    
    base_name = os.path.splitext(audio_filename)[0]
    txt_path = os.path.join(ASSETS_DIR, base_name + '.txt')
    
    if os.path.exists(txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except:
            return ""
    return ""

def load_model():
    global cosyvoice_model
    if cosyvoice_model is None:
        try:
            # å°è¯•é€‚é…ä¸åŒçš„ä»£ç ç‰ˆæœ¬
            try:
                from cosyvoice.cli.cosyvoice import CosyVoice2 as CosyVoiceCls
                print("Detected CosyVoice2 class.")
            except ImportError:
                from cosyvoice.cli.cosyvoice import CosyVoice as CosyVoiceCls
                print("Detected CosyVoice class.")

            # æŒ‡å‘åˆšä¸‹è½½çš„ CosyVoice2-0.5B
            model_dir = os.path.join(cosyvoice_root, 'pretrained_models', 'CosyVoice2-0.5B')
            print(f"Loading model from {model_dir}...")
            
            try:
                # å°è¯•åŠ è½½
                cosyvoice_model = CosyVoiceCls(model_dir, load_jit=False, load_trt=False, load_vllm=False, fp16=True)
                return "Model loaded successfully (FP16)."
            except Exception as e:
                print(f"FP16 load failed: {e}, trying FP32...")
                cosyvoice_model = CosyVoiceCls(model_dir, load_jit=False, load_trt=False, load_vllm=False, fp16=False)
                return "Model loaded successfully (FP32)."
        except Exception as e:
            return f"Error loading model: {str(e)}"
    return "Model already loaded."

def save_task_state(state):
    """ä¿å­˜ä»»åŠ¡çŠ¶æ€åˆ°JSONæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global task_state_lock
    try:
        with task_state_lock:
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶+åŸå­é‡å‘½åï¼Œç¡®ä¿å†™å…¥çš„åŸå­æ€§
            temp_file = TASK_STATE_FILE + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            # åŸå­é‡å‘½åï¼ˆåœ¨POSIXç³»ç»Ÿä¸Šæ˜¯åŸå­æ“ä½œï¼‰
            os.replace(temp_file, TASK_STATE_FILE)
    except Exception as e:
        print(f"Failed to save task state: {e}")

def load_task_state():
    """ä»JSONæ–‡ä»¶åŠ è½½ä»»åŠ¡çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global task_state_lock
    try:
        with task_state_lock:
            if os.path.exists(TASK_STATE_FILE):
                with open(TASK_STATE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
    except Exception as e:
        print(f"Failed to load task state: {e}")
    return None

def clear_task_state():
    """æ¸…é™¤ä»»åŠ¡çŠ¶æ€æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global task_state_lock
    try:
        with task_state_lock:
            if os.path.exists(TASK_STATE_FILE):
                os.remove(TASK_STATE_FILE)
    except Exception as e:
        print(f"Failed to clear task state: {e}")

def get_task_status():
    """è·å–å½“å‰ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºç•Œé¢åˆ·æ–°ï¼‰"""
    state = load_task_state()
    if not state:
        return "æš‚æ— è¿è¡Œä¸­çš„ä»»åŠ¡", None
    
    status = state.get('status', 'unknown')
    current_file = state.get('current_file', '')
    total_files = state.get('total_files', 0)
    file_idx = state.get('file_idx', 0)
    progress_pct = state.get('progress', 0) * 100
    message = state.get('message', '')
    generated_files = state.get('generated_files', [])
    
    if status == 'running':
        status_msg = f"ğŸŸ¢ ä»»åŠ¡è¿è¡Œä¸­\n"
        if total_files > 0:
            status_msg += f"è¿›åº¦: {file_idx + 1}/{total_files} æ–‡ä»¶ ({progress_pct:.1f}%)\n"
        if current_file:
            status_msg += f"å½“å‰æ–‡ä»¶: {current_file}\n"
        if message:
            status_msg += f"çŠ¶æ€: {message}"
    elif status == 'completed':
        status_msg = f"âœ… ä»»åŠ¡å·²å®Œæˆ\n"
        if message:
            status_msg += f"{message}"
        if generated_files:
            status_msg += f"\nå·²ç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶"
    elif status == 'stopped':
        status_msg = f"ğŸ›‘ ä»»åŠ¡å·²åœæ­¢\n"
        if message:
            status_msg += f"{message}"
    elif status == 'error':
        status_msg = f"âŒ ä»»åŠ¡å‡ºé”™\n"
        if message:
            status_msg += f"{message}"
    else:
        status_msg = f"çŠ¶æ€: {status}\n"
        if message:
            status_msg += f"{message}"
    
    # å¦‚æœæœ‰ç”Ÿæˆçš„æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶åˆ—è¡¨
    files = None
    if generated_files:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
        existing_files = []
        for f in generated_files:
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾
            if not os.path.isabs(f):
                full_path = os.path.join(current_dir, f)
                if os.path.exists(full_path):
                    existing_files.append(full_path)
            elif os.path.exists(f):
                existing_files.append(f)
        if existing_files:
            files = existing_files
    
    return status_msg, files

def _execute_conversion_task(text_files, ref_audio_name, prompt_text):
    """
    åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè½¬æ¢ä»»åŠ¡ï¼ˆä¸ä¾èµ– yieldï¼Œå³ä½¿å‰ç«¯å…³é—­ä¹Ÿèƒ½ç»§ç»­è¿è¡Œï¼‰
    """
    global cosyvoice_model, stop_flag, current_inference_thread
    
    current_inference_thread = threading.current_thread()
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    task_state = {
        'status': 'running',
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'current_file': '',
        'file_idx': 0,
        'total_files': 0,
        'progress': 0.0,
        'message': 'ä»»åŠ¡å¯åŠ¨ä¸­...',
        'generated_files': []
    }
    save_task_state(task_state)
    
    try:
        # 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        if cosyvoice_model is None:
            msg = "Loading model..."
            task_state['message'] = msg
            task_state['progress'] = 0.0
            save_task_state(task_state)
            msg = load_model()
            task_state['message'] = msg
            save_task_state(task_state)
            if cosyvoice_model is None:
                task_state['status'] = 'error'
                task_state['message'] = 'æ¨¡å‹åŠ è½½å¤±è´¥'
                save_task_state(task_state)
                return
            if stop_flag.is_set():
                msg = "è½¬æ¢å·²åœæ­¢"
                task_state['status'] = 'stopped'
                task_state['message'] = msg
                save_task_state(task_state)
                print("æ¨¡å‹åŠ è½½åæ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                _cleanup_model_immediate()
                return

        # 2. éªŒè¯è¾“å…¥
        msg = "Validating inputs..."
        task_state['message'] = msg
        task_state['progress'] = 0.1
        save_task_state(task_state)
        
        if not text_files:
            task_state['status'] = 'error'
            task_state['message'] = "Error: Please upload at least one text file."
            save_task_state(task_state)
            return
        
        if not ref_audio_name:
            task_state['status'] = 'error'
            task_state['message'] = "Error: Please select a reference audio."
            save_task_state(task_state)
            return

        ref_audio_path = os.path.join(ASSETS_DIR, ref_audio_name)
        if not os.path.exists(ref_audio_path):
            task_state['status'] = 'error'
            task_state['message'] = f"Error: Audio file not found: {ref_audio_path}"
            save_task_state(task_state)
            return

        # ç¡®ä¿ text_files æ˜¯åˆ—è¡¨
        if not isinstance(text_files, list):
            text_files = [text_files]
        
        total_files = len(text_files)
        all_generated_files = []
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_state['total_files'] = total_files
        task_state['generated_files'] = []
        save_task_state(task_state)

        for file_idx, text_file in enumerate(text_files):
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if stop_flag.is_set():
                msg = "è½¬æ¢å·²åœæ­¢"
                task_state['status'] = 'stopped'
                task_state['message'] = msg
                save_task_state(task_state)
                print("æ–‡ä»¶å¾ªç¯ä¸­æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                _cleanup_model_immediate()
                break
                
            file_name = os.path.basename(text_file.name)
            msg = f"Processing file {file_idx + 1}/{total_files}: {file_name}..."
            task_state['current_file'] = file_name
            task_state['file_idx'] = file_idx
            task_state['message'] = msg
            task_state['progress'] = file_idx / total_files
            save_task_state(task_state)

            # è¯»å–æ–‡æœ¬
            with open(text_file.name, 'r', encoding='utf-8') as f:
                full_text = f.read().strip()
            
            msg = f"File {file_idx + 1}/{total_files}: Text loaded ({len(full_text)} chars). Inferencing..."
            task_state['message'] = msg
            save_task_state(task_state)
            
            if stop_flag.is_set():
                msg = "è½¬æ¢å·²åœæ­¢"
                task_state['status'] = 'stopped'
                task_state['message'] = msg
                save_task_state(task_state)
                print("æ¨ç†å¼€å§‹å‰æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                _cleanup_model_immediate()
                break
            
            from cosyvoice.utils.file_utils import load_wav
            prompt_speech_16k = load_wav(ref_audio_path, 16000)

            start_time = time.time()
            
            # 3. åˆå§‹åŒ–æµå¼ç´¯ç§¯å˜é‡
            chunk_count = 0
            estimated_chunks = max(1, len(full_text) // 10)
            
            current_part_audio = []
            current_part_samples = 0
            part_index = 0
            MAX_DURATION_SEC = 45 * 60  # 45 minutes
            sample_rate = cosyvoice_model.sample_rate
            MAX_SAMPLES = MAX_DURATION_SEC * sample_rate
            
            PAUSE_DURATION_MS = 200
            pause_samples = int(sample_rate * PAUSE_DURATION_MS / 1000)
            silence = torch.zeros(1, pause_samples)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_filename = os.path.splitext(file_name)[0]

            # å®šä¹‰å†…éƒ¨ä¿å­˜å‡½æ•°
            def save_current_part(audio_chunks, p_idx):
                if not audio_chunks:
                    return
                
                # æ‹¼æ¥ç‰‡æ®µå’Œåœé¡¿
                audio_with_pauses = []
                for idx, chunk in enumerate(audio_chunks):
                    audio_with_pauses.append(chunk)
                    if idx < len(audio_chunks) - 1:
                        audio_with_pauses.append(silence)
                
                full_part_tensor = torch.cat(audio_with_pauses, dim=1)
                
                # ç”Ÿæˆæ–‡ä»¶å
                part_suffix = f"_part{p_idx + 1}"
                output_base = f"{base_filename}_{timestamp}{part_suffix}"
                temp_wav = os.path.join(current_dir, f"temp_{output_base}.wav")
                output_mp4 = f"{output_base}.mp4"
                mp4_path = os.path.join(OUTPUT_DIR, output_mp4)
                
                torchaudio.save(temp_wav, full_part_tensor, sample_rate)
                
                # FFmpeg è½¬æ¢
                msg = f"File {file_idx + 1}/{total_files}: Converting part {p_idx + 1} to video..."
                task_state['message'] = msg
                save_task_state(task_state)
                
                cmd = [
                    "ffmpeg", "-y",
                    "-f", "lavfi", "-i", "color=c=black:s=320x240:r=1",
                    "-i", temp_wav,
                    "-c:v", "libx264", "-tune", "stillimage", "-pix_fmt", "yuv420p", "-crf", "40", "-preset", "veryfast",
                    "-c:a", "aac", "-b:a", "64k",
                    "-shortest",
                    mp4_path
                ]
                
                process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                
                if os.path.exists(temp_wav):
                    os.remove(temp_wav)

                if process.returncode != 0:
                     stderr_text = process.stderr.decode(errors='ignore').strip()
                     error_msg = f"FFmpeg error part {p_idx + 1}: {stderr_text}"
                     print(error_msg)
                     msg = f"File {file_idx + 1}/{total_files}: Video generation failed for part {p_idx + 1}. Details: {stderr_text}"
                     task_state['message'] = msg
                     save_task_state(task_state)
                else:
                    all_generated_files.append(mp4_path)
                    task_state['generated_files'] = all_generated_files.copy()
                    save_task_state(task_state)

            try:
                for i, output in enumerate(cosyvoice_model.inference_zero_shot(full_text, prompt_text, prompt_speech_16k, stream=False)):
                    # æ£€æŸ¥åœæ­¢æ ‡å¿—
                    if stop_flag.is_set():
                        msg = "è½¬æ¢å·²åœæ­¢ï¼Œæ­£åœ¨æ¸…ç†èµ„æº..."
                        task_state['message'] = msg
                        save_task_state(task_state)
                        print("æ¨ç†è¿‡ç¨‹ä¸­æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                        _cleanup_model_immediate()
                        break
                    
                    chunk_count += 1
                    audio_chunk = output['tts_speech']
                    chunk_len = audio_chunk.shape[1]
                    duration = chunk_len / sample_rate
                    
                    msg = f"File {file_idx + 1}/{total_files}: Generated chunk {chunk_count} ({duration:.2f}s)..."
                    
                    # è¿›åº¦æ¡é€»è¾‘
                    file_progress = min(0.95, chunk_count / estimated_chunks)
                    global_progress = (file_idx + file_progress) / total_files
                    task_state['message'] = msg
                    task_state['progress'] = global_progress
                    save_task_state(task_state)
                    
                    # ç´¯ç§¯éŸ³é¢‘
                    if current_part_audio:
                        current_part_samples += pause_samples  # åªæœ‰åœ¨ç‰‡æ®µä¹‹é—´æ‰æ’å…¥åœé¡¿
                    current_part_audio.append(audio_chunk)
                    current_part_samples += chunk_len
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åˆ†æ®µé˜ˆå€¼
                    if current_part_samples >= MAX_SAMPLES:
                        save_current_part(current_part_audio, part_index)
                        part_index += 1
                        current_part_audio = []
                        current_part_samples = 0
                        
                        # å¼ºåˆ¶æ¸…ç†ä¸€ä¸‹å†…å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            
            except Exception as e:
                if stop_flag.is_set():
                    msg = "è½¬æ¢å·²åœæ­¢"
                    task_state['status'] = 'stopped'
                    task_state['message'] = msg
                    save_task_state(task_state)
                    print("æ¨ç†å¼‚å¸¸æ—¶æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                    _cleanup_model_immediate()
                    return
                raise

            if stop_flag.is_set():
                msg = "è½¬æ¢å·²åœæ­¢"
                task_state['status'] = 'stopped'
                task_state['message'] = msg
                save_task_state(task_state)
                print("æ¨ç†å®Œæˆåæ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ¸…ç†æ¨¡å‹...")
                _cleanup_model_immediate()
                return

            # ä¿å­˜å‰©ä½™çš„éƒ¨åˆ†
            if current_part_audio:
                save_current_part(current_part_audio, part_index)
            elif chunk_count == 0:
                msg = f"File {file_idx + 1}/{total_files}: Error: No audio generated for {file_name}"
                task_state['message'] = msg
                save_task_state(task_state)
            
            file_time = time.time() - start_time
            msg = f"File {file_idx + 1}/{total_files}: Done ({file_time:.2f}s)"
            task_state['message'] = msg
            task_state['progress'] = (file_idx + 1) / total_files
            task_state['generated_files'] = all_generated_files.copy()
            save_task_state(task_state)

        if stop_flag.is_set():
            msg = "è½¬æ¢å·²åœæ­¢"
            task_state['status'] = 'stopped'
            task_state['message'] = msg
            save_task_state(task_state)
            # æ³¨æ„ï¼šæ¨¡å‹æ¸…ç†åœ¨ finally å—ä¸­ç»Ÿä¸€å¤„ç†
        else:
            # æ˜¾ç¤ºæ–‡ä»¶åï¼ˆä¸åŒ…å«å®Œæ•´è·¯å¾„ï¼‰
            file_names = [os.path.basename(f) for f in all_generated_files]
            msg = f"All done! Generated {len(all_generated_files)} file(s) in output folder:\n" + "\n".join(file_names)
            task_state['status'] = 'completed'
            task_state['message'] = msg
            task_state['progress'] = 1.0
            task_state['generated_files'] = all_generated_files
            save_task_state(task_state)

    except Exception as e:
        if stop_flag.is_set():
            msg = "è½¬æ¢å·²åœæ­¢"
            task_state['status'] = 'stopped'
            task_state['message'] = msg
            save_task_state(task_state)
            # æ³¨æ„ï¼šæ¨¡å‹æ¸…ç†åœ¨ finally å—ä¸­ç»Ÿä¸€å¤„ç†
        else:
            import traceback
            error_trace = traceback.format_exc()
            print(error_trace)
            task_state['status'] = 'error'
            task_state['message'] = f"Error: {str(e)}"
            save_task_state(task_state)
    finally:
        # æ¸…ç†èµ„æºï¼ˆå¦‚æœåœæ­¢æ ‡å¿—è¢«è®¾ç½®ï¼Œç«‹å³æ¸…ç†æ¨¡å‹ï¼‰
        if stop_flag.is_set():
            print("æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œæ­£åœ¨æ¸…ç†æ¨¡å‹èµ„æº...")
            _cleanup_model_immediate()
        else:
            # æ­£å¸¸å®Œæˆæ—¶åªæ¸…ç†CUDAç¼“å­˜ï¼Œä¿ç•™æ¨¡å‹ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        # Bug 1 Fix: åªæœ‰åœ¨å½“å‰çº¿ç¨‹ä»ç„¶æ˜¯æ´»åŠ¨çº¿ç¨‹æ—¶æ‰é‡ç½®å¼•ç”¨
        # è¿™é˜²æ­¢äº†ç«æ€æ¡ä»¶ï¼šå¦‚æœæ–°ä»»åŠ¡å·²ç»å¯åŠ¨å¹¶è®¾ç½®äº† current_inference_threadï¼Œ
        # æ—§ä»»åŠ¡çš„ finally å—ä¸åº”è¯¥è¦†ç›–å®ƒ
        if current_inference_thread is threading.current_thread():
            current_inference_thread = None

def convert_book(text_files, ref_audio_name, prompt_text, progress=None):
    """
    å¯åŠ¨è½¬æ¢ä»»åŠ¡ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œå³ä½¿å‰ç«¯å…³é—­ä¹Ÿèƒ½ç»§ç»­è¿è¡Œï¼‰
    è¿™ä¸ªå‡½æ•°åªè´Ÿè´£å¯åŠ¨ä»»åŠ¡å¹¶å®šæœŸæŠ¥å‘ŠçŠ¶æ€
    """
    global background_task_thread, stop_flag, background_task_lock
    
    # Bug 1 Fix: ä½¿ç”¨é”åŒæ­¥è®¿é—® background_task_threadï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚å¯¼è‡´çº¿ç¨‹å¼•ç”¨è¢«è¦†ç›–
    # è¿™ç¡®ä¿å³ä½¿ default_concurrency_limit=2 å…è®¸å¹¶å‘è¯·æ±‚ï¼Œä¹Ÿåªæœ‰ä¸€ä¸ªä»»åŠ¡èƒ½å¯åŠ¨
    with background_task_lock:
        # å¦‚æœå·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å®ƒ
        if background_task_thread and background_task_thread.is_alive():
            stop_flag.set()
            background_task_thread.join(timeout=2)
            # Bug 2 Fix: å¦‚æœçº¿ç¨‹ä»åœ¨è¿è¡Œï¼Œä¿æŒåœæ­¢æ ‡å¿—è®¾ç½®ï¼Œä¸é‡ç½®
            # åªæœ‰åœ¨çº¿ç¨‹ç¡®å®å·²ç»“æŸæ—¶æ‰æ¸…é™¤æ ‡å¿—
            if background_task_thread.is_alive():
                # çº¿ç¨‹ä»åœ¨è¿è¡Œï¼Œä¿æŒåœæ­¢æ ‡å¿—è®¾ç½®
                # æ–°ä»»åŠ¡ä¸åº”è¯¥å¯åŠ¨ï¼Œå› ä¸ºæ—§ä»»åŠ¡è¿˜åœ¨è¿è¡Œ
                yield "Error: Previous task is still running. Please wait for it to stop or restart the application.", None
                return
            # çº¿ç¨‹å·²ç»“æŸï¼Œç°åœ¨å¯ä»¥å®‰å…¨åœ°æ¸…é™¤æ ‡å¿—
            stop_flag.clear()
        else:
            # æ²¡æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡ï¼Œç¡®ä¿æ ‡å¿—å·²æ¸…é™¤
            stop_flag.clear()
    
    # éªŒè¯è¾“å…¥
    if not text_files:
        yield "Error: Please upload at least one text file.", None
        return
    
    if not ref_audio_name:
        yield "Error: Please select a reference audio.", None
        return

    ref_audio_path = os.path.join(ASSETS_DIR, ref_audio_name)
    if not os.path.exists(ref_audio_path):
        yield f"Error: Audio file not found: {ref_audio_path}", None
        return
    
    # ä¿å­˜æ–‡ä»¶è·¯å¾„ï¼ˆå› ä¸º text_file.name å¯èƒ½åœ¨åå°çº¿ç¨‹ä¸­å¤±æ•ˆï¼‰
    if not isinstance(text_files, list):
        text_files = [text_files]
    
    # ä¿å­˜æ–‡ä»¶è·¯å¾„åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œä¾›åå°çº¿ç¨‹ä½¿ç”¨
    text_file_paths = []
    for text_file in text_files:
        # å¦‚æœæ˜¯æ–‡ä»¶å¯¹è±¡ï¼Œä¿å­˜è·¯å¾„
        if hasattr(text_file, 'name'):
            text_file_paths.append(text_file.name)
        else:
            text_file_paths.append(str(text_file))
    
    # å¯åŠ¨åå°ä»»åŠ¡çº¿ç¨‹
    def run_task():
        # é‡æ–°æ‰“å¼€æ–‡ä»¶ï¼ˆå› ä¸ºåŸå§‹æ–‡ä»¶å¯¹è±¡å¯èƒ½å·²å…³é—­ï¼‰
        file_objects = []
        for path in text_file_paths:
            if os.path.exists(path):
                # åˆ›å»ºä¸€ä¸ªç±»ä¼¼æ–‡ä»¶å¯¹è±¡çš„åŒ…è£…
                class FileWrapper:
                    def __init__(self, path):
                        self.name = path
                file_objects.append(FileWrapper(path))
        
        if file_objects:
            _execute_conversion_task(file_objects, ref_audio_name, prompt_text)
        else:
            # Bug 1 Fix: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé”™è¯¯
            task_state = {
                'status': 'error',
                'message': 'Error: No valid text files found. Files may have been deleted or moved.',
                'progress': 0.0,
                'generated_files': []
            }
            save_task_state(task_state)
    
    # Bug 1 Fix: åœ¨çº¿ç¨‹åˆ›å»ºã€èµ‹å€¼å’Œå¯åŠ¨æ—¶æŒæœ‰é”ï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚è¦†ç›– background_task_thread
    # è¿™ç¡®ä¿å³ä½¿ä¸¤ä¸ªè¯·æ±‚åŒæ—¶åˆ°è¾¾ï¼Œä¹Ÿåªæœ‰ä¸€ä¸ªèƒ½æˆåŠŸåˆ›å»ºå’Œå¯åŠ¨ä»»åŠ¡çº¿ç¨‹
    with background_task_lock:
        # å†æ¬¡æ£€æŸ¥ï¼ˆåœ¨é”å†…ï¼‰ï¼Œé˜²æ­¢åœ¨éªŒè¯è¾“å…¥æœŸé—´å¦ä¸€ä¸ªè¯·æ±‚å·²ç»å¯åŠ¨äº†ä»»åŠ¡
        if background_task_thread and background_task_thread.is_alive():
            yield "Error: Another task was started while validating inputs. Please wait for it to complete.", None
            return
        
        # åˆ›å»ºå¹¶èµ‹å€¼çº¿ç¨‹ï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
        background_task_thread = threading.Thread(target=run_task, daemon=False)
        # Bug 1 Fix: start() å¿…é¡»åœ¨é”å†…è°ƒç”¨ï¼Œé˜²æ­¢åœ¨é‡Šæ”¾é”å’Œå¯åŠ¨çº¿ç¨‹ä¹‹é—´
        # å¦ä¸€ä¸ªè¯·æ±‚è¦†ç›– background_task_threadï¼Œå¯¼è‡´å¯åŠ¨é”™è¯¯çš„çº¿ç¨‹
        background_task_thread.start()
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    task_state = {
        'status': 'running',
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'current_file': '',
        'file_idx': 0,
        'total_files': 0,
        'progress': 0.0,
        'message': 'ä»»åŠ¡å¯åŠ¨ä¸­...',
        'generated_files': []
    }
    save_task_state(task_state)
    
    yield "ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°è¿è¡Œ...", None
    
    # å®šæœŸæŠ¥å‘Šä»»åŠ¡çŠ¶æ€ï¼ˆå³ä½¿å‰ç«¯å…³é—­ï¼Œä»»åŠ¡ä¹Ÿä¼šåœ¨åå°ç»§ç»­è¿è¡Œï¼‰
    # è¿™ä¸ªå¾ªç¯ä½œä¸º fallbackï¼Œç¡®ä¿åœ¨ auto-refresh ä¸å¯ç”¨æ—¶ï¼ˆå¦‚æ—§ç‰ˆ Gradioï¼‰ä»èƒ½æä¾›æ›´æ–°
    # å³ä½¿ auto-refresh å¯ç”¨ï¼Œè¿™ä¸ªå¾ªç¯ä¹Ÿèƒ½æä¾›æ›´åŠæ—¶çš„æ›´æ–°
    last_status = None
    last_files = None
    while True:
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if background_task_thread and not background_task_thread.is_alive():
                # ä»»åŠ¡å·²å®Œæˆï¼Œè·å–æœ€ç»ˆçŠ¶æ€
                status_msg, files = get_task_status()
                if status_msg != last_status or files != last_files:
                    yield status_msg, files
                break
            
            # è·å–å½“å‰ä»»åŠ¡çŠ¶æ€
            status_msg, files = get_task_status()
            if status_msg != last_status or files != last_files:
                yield status_msg, files
                last_status = status_msg
                last_files = files
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆæˆ–å‡ºé”™
            task_state = load_task_state()
            if task_state and task_state.get('status') in ['completed', 'error', 'stopped']:
                # ä»»åŠ¡å·²å®Œæˆï¼Œç­‰å¾…çº¿ç¨‹ç»“æŸ
                if background_task_thread:
                    background_task_thread.join(timeout=1)
                # è·å–æœ€ç»ˆçŠ¶æ€
                status_msg, files = get_task_status()
                if status_msg != last_status or files != last_files:
                    yield status_msg, files
                break
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥ï¼ˆé¿å…è¿‡äºé¢‘ç¹ï¼‰
            time.sleep(1)
            
        except GeneratorExit:
            # å‰ç«¯å…³é—­ï¼Œä½†ä»»åŠ¡ç»§ç»­åœ¨åå°è¿è¡Œ
            # ä¸é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç”Ÿæˆå™¨æ­£å¸¸ç»“æŸ
            break
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•ä½†ä¸ä¸­æ–­ä»»åŠ¡
            print(f"Error in status reporting: {e}")
            time.sleep(1)

def refresh_audio_list():
    return gr.Dropdown(choices=get_reference_audio_list())

def _cleanup_model_immediate():
    """ç«‹å³æ¸…ç†æ¨¡å‹èµ„æºï¼ˆåŒæ­¥æ‰§è¡Œï¼Œç¡®ä¿GPUèµ„æºé‡Šæ”¾ï¼‰"""
    global cosyvoice_model
    import gc
    
    try:
        # å®‰å…¨åœ°æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        model_ref = None
        try:
            model_ref = cosyvoice_model
        except Exception:
            pass
        
        if model_ref is not None:
            try:
                # å°è¯•å°†æ¨¡å‹ç§»åˆ° CPUï¼ˆå¦‚æœæ”¯æŒï¼‰
                if hasattr(model_ref, 'to'):
                    try:
                        model_ref.to('cpu')
                        print("æ¨¡å‹å·²ç§»åˆ°CPU")
                    except Exception as e:
                        print(f"ç§»åŠ¨æ¨¡å‹åˆ°CPUæ—¶å‡ºç°è­¦å‘Š: {e}")
            except Exception as e:
                print(f"æ£€æŸ¥æ¨¡å‹ç§»åŠ¨æ–¹æ³•æ—¶å‡ºç°è­¦å‘Š: {e}")
            
            # å°è¯•æ¸…ç†æ¨¡å‹å†…éƒ¨èµ„æº
            try:
                if hasattr(model_ref, 'cpu'):
                    model_ref.cpu()
            except Exception:
                pass
            
            # åˆ é™¤æ¨¡å‹å¼•ç”¨
            try:
                del model_ref
            except Exception:
                pass
            
            # æ¸…é™¤å…¨å±€å¼•ç”¨
            try:
                cosyvoice_model = None
            except Exception:
                pass
            print("æ¨¡å‹å¼•ç”¨å·²æ¸…é™¤")
        
        # æ¸…ç† CUDA ç¼“å­˜
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
                print("CUDAç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†CUDAç¼“å­˜æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # åƒåœ¾å›æ”¶
        try:
            gc.collect()
            print("åƒåœ¾å›æ”¶å·²å®Œæˆ")
        except Exception as e:
            print(f"åƒåœ¾å›æ”¶æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # å†æ¬¡æ¸…ç† CUDAï¼ˆç¡®ä¿å½»åº•é‡Šæ”¾ï¼‰
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass
    except Exception as e:
        print(f"æ¸…ç†æ¨¡å‹æ—¶å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        # å³ä½¿å‡ºé”™ä¹Ÿå°è¯•æ¸…é™¤å…¨å±€å¼•ç”¨
        try:
            cosyvoice_model = None
        except Exception:
            pass

def _cleanup_model_background():
    """åœ¨åå°çº¿ç¨‹ä¸­æ¸…ç†æ¨¡å‹èµ„æºï¼ˆé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
    import time
    # å…ˆç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ä¸»å‡½æ•°å·²ç»è¿”å›
    time.sleep(0.2)
    # è°ƒç”¨ç«‹å³æ¸…ç†å‡½æ•°
    _cleanup_model_immediate()

def stop_conversion():
    """åœæ­¢è½¬æ¢å¹¶å¼ºåˆ¶æ¸…ç†èµ„æºï¼ŒåŒ…æ‹¬å¸è½½æ¨¡å‹ - å¿«é€Ÿè¿”å›ç‰ˆæœ¬"""
    global stop_flag
    
    # ç”¨ try-except åŒ…è£¹æ•´ä¸ªå‡½æ•°ï¼Œæ•è·æ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸
    try:
        # ç¬¬ä¸€æ­¥ï¼šç«‹å³è®¾ç½®åœæ­¢æ ‡å¿—ï¼ˆæœ€å¿«æ“ä½œï¼‰
        try:
            if stop_flag is not None:
                stop_flag.set()
        except Exception as e:
            print(f"ERROR in stop_conversion (stop_flag.set): {e}")
            # Don't re-raise - continue execution to return message
        
        # ç¬¬äºŒæ­¥ï¼šç«‹å³è¿”å›æ¶ˆæ¯ï¼ˆä¸ç­‰å¾…ä»»ä½•å…¶ä»–æ“ä½œï¼‰
        result_msg = "ğŸ›‘ è½¬æ¢å·²åœæ­¢ï¼Œæ­£åœ¨æ¸…ç†èµ„æº..."
        
        # ç¬¬ä¸‰æ­¥ï¼šæ‰€æœ‰å…¶ä»–æ“ä½œéƒ½åœ¨åå°å¼‚æ­¥æ‰§è¡Œ
        def async_operations():
            """å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰å¯èƒ½é˜»å¡çš„æ“ä½œ"""
            global cosyvoice_model
            try:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                try:
                    task_state = load_task_state()
                    if task_state is not None and isinstance(task_state, dict):
                        task_state['status'] = 'stopped'
                        task_state['message'] = result_msg
                        save_task_state(task_state)
                except Exception as e:
                    print(f"ERROR in async_operations (task_state): {e}")
                    pass
                
                # æ‰§è¡Œæ¨¡å‹æ¸…ç†ï¼ˆç«‹å³æ¸…ç†ï¼Œä¸ç­‰å¾…ï¼‰
                # æ³¨æ„ï¼šè½¬æ¢ä»»åŠ¡ä¹Ÿä¼šåœ¨æ£€æµ‹åˆ° stop_flag æ—¶æ¸…ç†æ¨¡å‹ï¼Œè¿™é‡Œæ˜¯åŒé‡ä¿é™©
                print("stop_conversion: å¼€å§‹åå°æ¸…ç†æ¨¡å‹...")
                _cleanup_model_immediate()
            except Exception as e:
                print(f"åå°æ“ä½œæ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹ï¼ˆä¸ç­‰å¾…ï¼‰
        try:
            thread = threading.Thread(target=async_operations, daemon=True)
            thread.start()
        except Exception as e:
            print(f"ERROR in stop_conversion (thread start): {e}")
            pass
        
        # ç«‹å³è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸è¿”å›æ¶ˆæ¯ï¼Œå› ä¸º outputs=[]ï¼Œæ¶ˆæ¯é€šè¿‡ task_state æ›´æ–°æ˜¾ç¤ºï¼‰
        return []
    
    except Exception as e:
        # æ•è·æ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸
        # #region agent log
        try:
            with open(log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps({"id":"log_stop_013","timestamp":int(time.time()*1000),"location":"web_book_converter.py:805","message":"stop_conversion unhandled exception","data":{"error":str(e),"traceback":traceback.format_exc()},"sessionId":"debug-session","runId":"run2","hypothesisId":"A"})+'\n')
        except Exception as log_err:
            print(f"DEBUG LOG ERROR (unhandled exception): {log_err}")
        # #endregion
        print(f"CRITICAL ERROR in stop_conversion: {e}")
        traceback.print_exc()
        # è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸ outputs=[] é…ç½®ä¸€è‡´ï¼Œé¿å… Gradio è¿æ¥é”™è¯¯ï¼‰
        # #region agent log
        try:
            with open(log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps({"id":"log_stop_013b","timestamp":int(time.time()*1000),"location":"web_book_converter.py:868","message":"returning empty list from exception handler","data":{"error":str(e)},"sessionId":"debug-session","runId":"run5","hypothesisId":"G,B"})+'\n')
        except Exception as log_err:
            print(f"DEBUG LOG ERROR (exception return): {log_err}")
        # #endregion
        return []  # å‡è®¾G: è¿”å›ç©ºåˆ—è¡¨ä¸ outputs=[] ä¸€è‡´

# Gradio ç•Œé¢æ„å»º
custom_css = """
    /* ä¸»å®¹å™¨è‡ªé€‚åº” */
    .gradio-container {
        max-width: 100% !important;
        width: 100% !important;
        height: 100vh !important;
        max-height: 100vh !important;
        display: flex !important;
        flex-direction: column !important;
        overflow: hidden !important;
    }
    
    .main-container {
        padding: 0.3rem !important;
        flex: 1 1 auto !important;
        display: flex !important;
        flex-direction: column !important;
        overflow-y: auto !important;
        overflow-x: hidden !important;
        min-height: 0 !important;
        max-height: 100% !important;
    }
    
    /* æ ‡é¢˜è‡ªé€‚åº” */
    .markdown {
        margin: 0.2rem 0 !important;
        font-size: clamp(0.9rem, 2vw, 1.1rem) !important;
        flex-shrink: 0 !important;
    }
    
    /* è¡¨å•å…ƒç´ è‡ªé€‚åº” */
    .form {
        margin-bottom: 0.3rem !important;
        flex-shrink: 0 !important;
    }
    
    .panel {
        margin-bottom: 0.3rem !important;
        flex-shrink: 0 !important;
    }
    
    /* æ–‡ä»¶ä¸Šä¼ åŒºåŸŸè‡ªé€‚åº” */
    .file-upload-area {
        min-height: 60px !important;
        height: auto !important;
        max-height: 150px !important;
        overflow-y: auto !important;
    }
    
    /* æ–‡æœ¬æ¡†è‡ªé€‚åº” */
    .textbox {
        min-height: auto !important;
        height: auto !important;
        resize: vertical !important;
    }
    
    /* æŒ‰é’®è‡ªé€‚åº” */
    .button {
        height: auto !important;
        min-height: 32px !important;
        padding: 0.3rem 0.8rem !important;
        white-space: nowrap !important;
    }
    
    .accordion {
        margin-bottom: 0.3rem !important;
        flex-shrink: 0 !important;
    }
    
    /* è¡Œå¸ƒå±€è‡ªé€‚åº” */
    .gradio-row {
        flex-wrap: wrap !important;
        gap: 0.5rem !important;
        align-items: flex-start !important;
    }
    
    /* åˆ—å¸ƒå±€è‡ªé€‚åº” */
    .gradio-column {
        display: flex !important;
        flex-direction: column !important;
        min-height: fit-content !important;
        height: auto !important;
        flex: 1 1 auto !important;
    }
    
    /* è¾“å‡ºæ–‡æœ¬æ¡†è‡ªé€‚åº” */
    .output-textbox {
        flex: 1 1 auto !important;
        min-height: 100px !important;
        max-height: 40vh !important;
        overflow-y: auto !important;
        resize: vertical !important;
    }
    
    /* æ–‡ä»¶ä¸‹è½½åŒºåŸŸè‡ªé€‚åº” */
    .file-download {
        flex-shrink: 0 !important;
        max-height: 25vh !important;
        overflow-y: auto !important;
        min-height: 60px !important;
    }
    
    /* å“åº”å¼å¸ƒå±€ï¼šå°å±å¹•æ—¶ä¸Šä¸‹å †å  */
    @media (max-width: 768px) {
        .gradio-row {
            flex-direction: column !important;
        }
        
        .gradio-column {
            width: 100% !important;
            min-width: 100% !important;
            max-width: 100% !important;
        }
        
        .output-textbox {
            max-height: 30vh !important;
        }
        
        .file-download {
            max-height: 20vh !important;
        }
    }
    
    /* è¶…å°å±å¹•ä¼˜åŒ– */
    @media (max-width: 480px) {
        .main-container {
            padding: 0.2rem !important;
        }
        
        .button {
            min-height: 36px !important;
            font-size: 0.9rem !important;
        }
        
        .output-textbox {
            max-height: 25vh !important;
        }
    }
    
    /* é«˜å±å¹•ä¼˜åŒ– */
    @media (min-height: 900px) {
        .output-textbox {
            max-height: 50vh !important;
        }
    }
"""

with gr.Blocks(title="CosyVoice Book Converter", theme=gr.themes.Soft(), css=custom_css) as demo:
    gr.Markdown("### ğŸ“š CosyVoice æœ‰å£°ä¹¦è½¬æ¢å™¨")
    
    with gr.Row(equal_height=False):
        with gr.Column(scale=1, min_width=280):
            text_input = gr.File(
                label="ä¸Šä¼ ä¹¦ç± (.txt)", 
                file_types=[".txt"], 
                file_count="multiple"
            )
            
            with gr.Accordion("å‚è€ƒéŸ³é¢‘è®¾ç½®", open=False):
                with gr.Row():
                    ref_audio_dropdown = gr.Dropdown(
                        label="å‚è€ƒéŸ³é¢‘", 
                        choices=get_reference_audio_list(),
                        value=get_reference_audio_list()[0] if get_reference_audio_list() else None,
                        interactive=True,
                        scale=4,
                        container=False
                    )
                    refresh_btn = gr.Button("ğŸ”„", size="sm", scale=1, min_width=40)
                
                prompt_text_input = gr.Textbox(
                    label="Prompt Text", 
                    lines=1,
                    placeholder="é€‰æ‹©éŸ³é¢‘åè‡ªåŠ¨å¡«å……...",
                    max_lines=1,
                    container=False
                )
            
            with gr.Row():
                convert_btn = gr.Button("å¼€å§‹è½¬æ¢", variant="primary", scale=1, size="sm")
                stop_btn = gr.Button("åœæ­¢è½¬æ¢", variant="stop", scale=1, size="sm")
        
        with gr.Column(scale=1, min_width=280):
            with gr.Row():
                log_output = gr.Textbox(
                    label="è¿è¡Œæ—¥å¿—", 
                    lines=6, 
                    interactive=False,
                    show_copy_button=True,
                    container=False,
                    elem_classes=["output-textbox"],
                    scale=4
                )
                refresh_log_btn = gr.Button("ğŸ”„ åˆ·æ–°æ—¥å¿—", size="sm", scale=1, min_width=80)
            files_output = gr.File(
                label="ç”Ÿæˆæ–‡ä»¶ä¸‹è½½", 
                file_count="multiple", 
                interactive=False,
                elem_classes=["file-download"]
            )

    # äº‹ä»¶ç»‘å®š
    refresh_btn.click(fn=refresh_audio_list, inputs=[], outputs=ref_audio_dropdown)
    
    # é€‰æ‹©éŸ³é¢‘æ—¶è‡ªåŠ¨æ›´æ–° prompt text
    ref_audio_dropdown.change(
        fn=get_prompt_text_for_audio,
        inputs=[ref_audio_dropdown],
        outputs=[prompt_text_input]
    )

    submit_event = convert_btn.click(
        fn=convert_book,
        inputs=[text_input, ref_audio_dropdown, prompt_text_input],
        outputs=[log_output, files_output]
    )
    
    # åœæ­¢æŒ‰é’®ï¼šè°ƒç”¨åœæ­¢å‡½æ•°å¹¶å–æ¶ˆäº‹ä»¶
    # ä½¿ç”¨ show_progress=False ç¡®ä¿ç«‹å³å“åº”
    # ä¸è¾“å‡ºåˆ° log_output ä»¥é¿å…ä¸ convert_book çš„å¹¶å‘æ›´æ–°å†²çª
    # åœæ­¢æ¶ˆæ¯ä¼šé€šè¿‡ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼Œç”± get_task_status è‡ªåŠ¨æ˜¾ç¤º
    # 
    # ä¿®å¤ï¼šåˆ›å»ºåŒ…è£…å‡½æ•°ä»¥ç¡®ä¿ Gradio æ­£ç¡®å¤„ç†è¿”å›å€¼
    def stop_conversion_wrapper():
        """åŒ…è£…å‡½æ•°ï¼šç¡®ä¿ Gradio æ­£ç¡®å¤„ç†åœæ­¢æ“ä½œ"""
        try:
            stop_conversion()
            return []  # outputs=[] æ—¶åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
        except Exception as e:
            print(f"ERROR in stop_conversion_wrapper: {e}")
            import traceback
            traceback.print_exc()
            return []  # å³ä½¿å¼‚å¸¸ä¹Ÿè¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸ outputs=[] ä¸€è‡´ï¼‰
    
    # ä¿®å¤ï¼šä½¿ç”¨ inputs=[] å’Œ outputs=[] è€Œä¸æ˜¯ Noneï¼Œç¡®ä¿ Gradio æ­£ç¡®å¤„ç†è¿”å›å€¼
    stop_btn.click(
        fn=stop_conversion_wrapper,
        inputs=[],
        outputs=[],
        show_progress=False
    )
    
    # åˆ·æ–°æ—¥å¿—æŒ‰é’® - åŒæ—¶åˆ·æ–°ä»»åŠ¡çŠ¶æ€
    refresh_log_btn.click(
        fn=get_task_status,
        inputs=None,
        outputs=[log_output, files_output]
    )
    
    # åˆå§‹åŒ–æ—¶å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªéŸ³é¢‘çš„æ–‡æœ¬ï¼Œå¹¶æ¢å¤ä»»åŠ¡çŠ¶æ€
    def init_ui(ref_audio):
        """åˆå§‹åŒ–ç•Œé¢ï¼šåŠ è½½éŸ³é¢‘æ–‡æœ¬å’Œæ¢å¤ä»»åŠ¡çŠ¶æ€"""
        prompt_text = get_prompt_text_for_audio(ref_audio)
        status_msg, files = get_task_status()
        return prompt_text, status_msg, files
    
    demo.load(
        fn=init_ui,
        inputs=[ref_audio_dropdown], 
        outputs=[prompt_text_input, log_output, files_output]
    )
    
    # æ³¨æ„ï¼šè‡ªåŠ¨åˆ·æ–°åŠŸèƒ½éœ€è¦ Gradio 4.0+ï¼Œå¦‚æœç‰ˆæœ¬ä¸æ”¯æŒä¼šæŠ¥é”™
    # å·²æä¾›æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç‚¹å‡»"ğŸ”„ åˆ·æ–°æ—¥å¿—"æŒ‰é’®æ¥æŸ¥çœ‹æœ€æ–°çŠ¶æ€
    # å¦‚æœéœ€è¦è‡ªåŠ¨åˆ·æ–°ï¼Œè¯·å‡çº§ Gradio: pip install --upgrade gradio>=4.0.0
    try:
        demo.load(
            fn=get_task_status,
            inputs=None,
            outputs=[log_output, files_output],
            every=5  # æ¯5ç§’è‡ªåŠ¨åˆ·æ–°ï¼ˆéœ€è¦ Gradio 4.0+ï¼‰
        )
    except TypeError:
        # Gradio ç‰ˆæœ¬ä¸æ”¯æŒ every å‚æ•°ï¼Œè·³è¿‡è‡ªåŠ¨åˆ·æ–°
        # ç”¨æˆ·å¯ä»¥ä½¿ç”¨æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®
        pass

if __name__ == "__main__":
    print("Starting Web UI...")
    # ä½¿ç”¨ 0.0.0.0 è®©æœåŠ¡åœ¨æ‰€æœ‰ç½‘ç»œæ¥å£ä¸Šç›‘å¬
    import gradio
    # å°è¯•ç¦ç”¨ localhost æ£€æŸ¥
    gradio.strings.en["SHARE_LINK_MESSAGE"] = ""
    try:
        # ä½¿ç”¨ queue() å¯ç”¨ä»»åŠ¡é˜Ÿåˆ—ï¼Œç¡®ä¿ä»»åŠ¡åœ¨åå°ç»§ç»­è¿è¡Œ
        # max_size=1 ç¡®ä¿åªæœ‰ä¸€ä¸ªä»»åŠ¡åœ¨è¿è¡Œ
        # Gradio çš„ queue() é»˜è®¤æ”¯æŒåå°ä»»åŠ¡ï¼Œå³ä½¿å‰ç«¯å…³é—­ä¹Ÿä¸ä¼šä¸­æ–­
        # å¢åŠ é˜Ÿåˆ—å¤§å°ï¼Œå…è®¸å…¶ä»–è¯·æ±‚ï¼ˆå¦‚åœæ­¢ã€åˆ·æ–°ï¼‰åœ¨å¤„ç†è½¬æ¢ä»»åŠ¡æ—¶ä¹Ÿèƒ½å“åº”
        # max_size=3 å…è®¸æœ€å¤š 3 ä¸ªå¹¶å‘è¯·æ±‚ï¼Œç¡®ä¿åœæ­¢æŒ‰é’®å’Œåˆ·æ–°æŒ‰é’®å¯ä»¥å“åº”
        demo.queue(max_size=3, default_concurrency_limit=2).launch(
            server_name="0.0.0.0", 
            server_port=7860,
            show_error=True,
            quiet=False,
            inbrowser=False
        )
    except ValueError as e:
        if "shareable link" in str(e):
            print("Fallback: Using share=True due to network restrictions")
            # å¢åŠ é˜Ÿåˆ—å¤§å°ï¼Œå…è®¸å…¶ä»–è¯·æ±‚ï¼ˆå¦‚åœæ­¢ã€åˆ·æ–°ï¼‰åœ¨å¤„ç†è½¬æ¢ä»»åŠ¡æ—¶ä¹Ÿèƒ½å“åº”
            demo.queue(max_size=3, default_concurrency_limit=2).launch(
                server_name="0.0.0.0", 
                server_port=7860, 
                show_error=True, 
                share=True,
                inbrowser=False
            )